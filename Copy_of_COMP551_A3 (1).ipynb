{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "ciTCdnWb3gYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "y-VjhU2JdxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Acquire the Data"
      ],
      "metadata": {
        "id": "Mbh8EX80ust0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ],
      "metadata": {
        "id": "wKxbO43KQsEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb8635a-625c-432e-acd2-f3abe8578f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, c):\n",
        "\n",
        "    # y--> label/ground truth.\n",
        "    # c--> Number of classes.\n",
        "\n",
        "    # A zero matrix of size (m, c)\n",
        "    y_hot = np.zeros((len(y), c))\n",
        "\n",
        "    # Putting 1 for column where the label is,\n",
        "    # Using multidimensional indexing.\n",
        "    y_hot[np.arange(len(y)), y] = 1\n",
        "\n",
        "    return y_hot\n",
        "\n"
      ],
      "metadata": {
        "id": "hjoikfepA4KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorize\n",
        "x_train_f = np.reshape(x_train, (60000,  784))\n",
        "x_test_f = np.reshape(x_test, (10000,  784))\n",
        "\n",
        "x_train_f=x_train_f[0:1000]\n",
        "y_train=y_train[0:1000]\n",
        "x_test_f=x_test_f[0:400]\n",
        "y_test=y_test[0:400]\n"
      ],
      "metadata": {
        "id": "1BHgmQWoeXLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize\n",
        "def norm(arr):\n",
        "  mean = np.mean(arr, axis = 0)\n",
        "  arr = np.subtract(arr,mean)\n",
        "  std = np.std(arr, axis = 0)\n",
        "  arr = np.divide(arr,std)\n",
        "  return arr\n",
        "\n",
        "#x_train_fn = norm(x_train_f)\n",
        "x_train_fn=x_train_f/255\n",
        "#x_test_fn = norm(x_test_f)\n",
        "x_test_fn=x_test_f/255\n"
      ],
      "metadata": {
        "id": "b-Qyo-HGm0RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Implement a Multilayer Perceptron"
      ],
      "metadata": {
        "id": "FfzydHqzuzxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "\n",
        "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def run(self, gradient_fn, x, y, params):\n",
        "        norms = np.array([np.inf])\n",
        "        t = 1\n",
        "        losses=[]\n",
        "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
        "            grad, yh= gradient_fn(x, y, params)\n",
        "\n",
        "            momentum = 0.9                         # momentum\n",
        "\n",
        "            for p in range(len(grad)):\n",
        "                #update_p = np.zeros(grad[p].shape)\n",
        "\n",
        "                params[p]-= self.learning_rate * grad[p]\n",
        "\n",
        "#Using SGD with Nesterov acceleration Optimizer to update the weights and biases: https://medium.com/@neuralthreads/l1-l2-regularization-adding-penalties-to-the-loss-function-b5c330d30b3f\n",
        "\n",
        "                # update_p= -self.learning_rate * grad[p] + momentum * update_p\n",
        "                # update_p_= -self.learning_rate * grad[p] + momentum * update_p\n",
        "                # params[p]+= update_p_\n",
        "\n",
        "            t += 1\n",
        "            loss = -np.mean(np.log(yh[np.arange(len(y)), y]))\n",
        "            if t%100==0:\n",
        "\n",
        "              print('Epoch {epoch}==> Loss = {loss}' .format(epoch=t, loss=loss))\n",
        "              losses.append(loss)\n",
        "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
        "        return params , losses"
      ],
      "metadata": {
        "id": "D855sXeXIN5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful functions\n",
        "# ReLu\n",
        "def relu(x):\n",
        "  return(np.maximum(0,x))\n",
        "\n",
        "# Three different Softmax\n",
        "def stable_softmax(x):\n",
        "    z = x - np.max(x, axis=-1, keepdims=True)\n",
        "    numerator = np.exp(z)\n",
        "    denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
        "    softmax = numerator / denominator\n",
        "    return softmax\n",
        "\n",
        "def softmax(X):\n",
        "    X_exp = np.exp(X)\n",
        "    partition = X_exp.sum(1, keepdims=True)\n",
        "    return X_exp / partition\n",
        "\n",
        "def logsoftmax(x, recover_probs=True):  #This gives us the most stable softmax! No underflowing or overflowing. Theory explained here: https://ogunlao.github.io/2020/04/26/you_dont_really_know_softmax.html\n",
        "\n",
        "    max_x = np.max(x)\n",
        "    exp_x = np.exp(x - max_x)\n",
        "    sum_exp_x = np.sum(exp_x)\n",
        "    log_sum_exp_x = np.log(sum_exp_x)\n",
        "    max_plus_log_sum_exp_x = max_x + log_sum_exp_x\n",
        "    log_probs = x - max_plus_log_sum_exp_x\n",
        "\n",
        "    # Recover probs\n",
        "    if recover_probs:\n",
        "        exp_log_probs = np.exp(log_probs)\n",
        "        sum_log_probs = np.sum(exp_log_probs)\n",
        "        probs = exp_log_probs / sum_log_probs\n",
        "        return probs\n",
        "\n",
        "    return log_probs\n",
        "\n",
        "# tanh\n",
        "def tanh_act(x):\n",
        "  return (np.tanh(x))\n",
        "\n",
        "# Leaky ReLu\n",
        "def leaky_relu(x, l):\n",
        "  return (np.maximum(0, x) + l * np.minimum(0, x))\n",
        "\n",
        "#Relu derivative\n",
        "def reluDerivative(x):\n",
        "     x[x<=0] = 0\n",
        "     x[x>0] = 1\n",
        "     return x\n",
        "\n",
        "# #Cross entropy\n",
        "# def cross_E(y_true, y_pred):                    # CE\n",
        "#     return -np.sum(y_true * np.log(y_pred + 10**-100))\n",
        "# def cross_E_grad(y_true, y_pred):               # CE derivative\n",
        "#     return -y_true/(y_pred + 10**-100)"
      ],
      "metadata": {
        "id": "jBwi3dR_ZFVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP:\n",
        "\n",
        "    def __init__(self, hidden_units, HL, activation):\n",
        "        self.M = hidden_units\n",
        "        self.HL = HL\n",
        "        self.activation = activation\n",
        "\n",
        "        dv_sum=np.zeros((784, self.M))\n",
        "    def fit(self, x, y, optimizer):\n",
        "        # D: number of features\n",
        "        # N: number of samples\n",
        "        N, D = x.shape\n",
        "\n",
        "        def gradient(x, y, params):\n",
        "\n",
        "          if (self.HL==0):\n",
        "            v,b1= params\n",
        "            yh = stable_softmax(np.dot(x,v)+b1)  #N x M\n",
        "            y_hot=one_hot(y,10)\n",
        "            dy = yh-y_hot\n",
        "            dv = np.dot(x.T, dy)/N\n",
        "            dparams = [dv, b1]\n",
        "            return dparams, yh\n",
        "\n",
        "          elif (self.HL==1):\n",
        "            v, w ,b1,b2= params\n",
        "            z = self.activation(np.dot(x, v)+b1) #N x C\n",
        "            yh =stable_softmax(np.dot(z, w)+b2)#N\n",
        "            dy = (yh - one_hot(y,10))\n",
        "            dw = np.dot(z.T, dy)/N  #M x C\n",
        "            dv_sum=np.zeros((D, self.M))\n",
        "\n",
        "            for i in range (0,10):\n",
        "              dz = np.outer(dy[:,i], w[:,i])\n",
        "              dv = np.dot(x.T, dz * reluDerivative(z))/N\n",
        "              dv_sum = dv_sum+dv\n",
        "\n",
        "            dparams = [dv_sum, dw, b1, b2]\n",
        "            return dparams , yh\n",
        "\n",
        "          elif (self.HL==2):\n",
        "            v, w ,p ,b1 ,b2 ,b3 = params\n",
        "            z = self.activation(np.dot(x, v)+b1)    #N x C\n",
        "            k = self.activation(np.dot(z, w)+b2)\n",
        "            yh = stable_softmax(np.dot(k, p)+b3)     #N\n",
        "\n",
        "            dy = (yh - one_hot(y,10))\n",
        "            dp = np.dot(k.T, dy)/N        #M x C\n",
        "            dw_sum=np.zeros((self.M, self.M))\n",
        "\n",
        "            for i in range (0,10):\n",
        "              dk=np.outer(dy[:,i], p[:,i])\n",
        "              dw = np.dot(z.T, dk * reluDerivative(k))/N\n",
        "              dw_sum=dw_sum+dw\n",
        "\n",
        "            dv_sum=np.zeros((D, self.M))\n",
        "\n",
        "            for i in range (0,10):\n",
        "              dz = np.outer(dk[:,i], w[:,i])\n",
        "              dv = np.dot(x.T, dz * reluDerivative(z))/N\n",
        "              dv_sum = dv_sum+dv\n",
        "\n",
        "            dparams = [dv_sum, dw_sum, dp , b1, b2,b3]\n",
        "            return dparams , yh\n",
        "\n",
        "        if (self.HL==0):\n",
        "          v = np.random.randn(D, 10) * .01\n",
        "          b1 = np.random.randn(1, 10) *.01\n",
        "          params0 = [v,b1]\n",
        "          self.params, losses = optimizer.run(gradient, x, y, params0)\n",
        "\n",
        "        elif (self.HL==1):\n",
        "          w = np.random.randn(self.M,10) * .01 # changed from 0.1 to math.sqrt(self.M)\n",
        "          v = np.random.randn(D,self.M) * .01\n",
        "          b1 = np.random.randn(1, self.M) *.01\n",
        "          b2 = np.random.randn(1, 10) *.01\n",
        "          params0 = [v,w,b1,b2]\n",
        "          self.params, losses = optimizer.run(gradient, x, y, params0)\n",
        "\n",
        "        elif (self.HL==2):\n",
        "          p = np.random.randn(self.M, 10) * .01\n",
        "          w = np.random.randn(self.M, self.M) * .01\n",
        "          v = np.random.randn(D, self.M) * .01\n",
        "          b1 = np.random.randn(1, self.M) *.01\n",
        "          b2 = np.random.randn(1, self.M) *.01\n",
        "          b3 = np.random.randn(1, 10) *.01\n",
        "          params0 = [v,w,p,b1,b2,b3]\n",
        "          self.params, losses = optimizer.run(gradient, x, y, params0)\n",
        "\n",
        "        return self, losses\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "      if (self.HL==1):\n",
        "        v, w , b1, b2= self.params\n",
        "        z = self.activation(np.dot(x, v)+b1) #N x M\n",
        "        yh = stable_softmax(np.dot(z, w)+b2)#N\n",
        "\n",
        "      elif (self.HL==0):\n",
        "        v,b1=self.params\n",
        "        yh=stable_softmax(np.dot(x,v)+b1)\n",
        "\n",
        "      elif (self.HL==2):\n",
        "        v, w, p, b1, b2, b3 = self.params\n",
        "        z = self.activation(np.dot(x, v)+b1) #N x M\n",
        "        k = self.activation(np.dot(z, w)+b2) #M x M\n",
        "        yh = stable_softmax(np.dot(k, p)+b3)  #N\n",
        "\n",
        "      return np.argmax(yh, axis=1)\n"
      ],
      "metadata": {
        "id": "vwqBo6FYPL6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#swith 4000\n",
        "model = MLP(hidden_units=64, HL=2, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=4000)\n",
        "output, losses_2 = model.fit(x_train_fn, y_train, optimizer)\n",
        "\n",
        "yh= output.predict(x_test_fn)\n"
      ],
      "metadata": {
        "id": "t8COoWtnJmVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27708c98-d775-4885-cf9d-d25be7e08c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100==> Loss = 2.3020973009466354\n",
            "Epoch 200==> Loss = 2.301132763925152\n",
            "Epoch 300==> Loss = 2.29675791759154\n",
            "Epoch 400==> Loss = 2.1737323019188564\n",
            "Epoch 500==> Loss = 1.9103757437126843\n",
            "Epoch 600==> Loss = 1.8909053047789421\n",
            "Epoch 700==> Loss = 1.8725384997831924\n",
            "Epoch 800==> Loss = 1.843791758074598\n",
            "Epoch 900==> Loss = 1.7998864417412992\n",
            "Epoch 1000==> Loss = 1.7153661273852854\n",
            "Epoch 1100==> Loss = 1.5934241246189953\n",
            "Epoch 1200==> Loss = 1.4908926184181173\n",
            "Epoch 1300==> Loss = 1.4311755245889475\n",
            "Epoch 1400==> Loss = 1.3976332543002083\n",
            "Epoch 1500==> Loss = 1.3721931363093913\n",
            "Epoch 1600==> Loss = 1.3467147671212514\n",
            "Epoch 1700==> Loss = 1.31875604681474\n",
            "Epoch 1800==> Loss = 1.2880675458020376\n",
            "Epoch 1900==> Loss = 1.25678862921562\n",
            "Epoch 2000==> Loss = 1.2289277305716941\n",
            "Epoch 2100==> Loss = 1.225612070991332\n",
            "Epoch 2200==> Loss = 1.208850049366838\n",
            "Epoch 2300==> Loss = 1.1948454418182457\n",
            "Epoch 2400==> Loss = 1.1838679986364087\n",
            "Epoch 2500==> Loss = 1.1700367252824417\n",
            "Epoch 2600==> Loss = 1.1615338306602776\n",
            "Epoch 2700==> Loss = 1.1461943908596568\n",
            "Epoch 2800==> Loss = 1.1150794116638385\n",
            "Epoch 2900==> Loss = 1.1024820715704873\n",
            "Epoch 3000==> Loss = 1.0718977909243785\n",
            "Epoch 3100==> Loss = 1.0460491330919057\n",
            "Epoch 3200==> Loss = 1.0229628081444773\n",
            "Epoch 3300==> Loss = 1.0085244816650305\n",
            "Epoch 3400==> Loss = 0.9879241949568853\n",
            "Epoch 3500==> Loss = 1.1423438780493744\n",
            "Epoch 3600==> Loss = 0.9584461490334979\n",
            "Epoch 3700==> Loss = 0.9833628675775467\n",
            "Epoch 3800==> Loss = 0.9399450383650846\n",
            "Epoch 3900==> Loss = 0.9589756476273578\n",
            "Epoch 4000==> Loss = 0.9081553054763623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(yh)\n",
        "\n",
        "print(losses_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZvj3OymTgeR",
        "outputId": "539d7371-3b06-44d9-8e27-4ed7a25ac4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 2 1 1 4 1 6 4 7 7 4 7 7 3 4 1 2 6 8 0 2 7 7 7 1 2 4 3 9 1 8 0 3 1 8 0 7\n",
            " 7 7 9 0 1 3 7 6 9 6 1 4 4 2 4 7 2 6 2 8 4 8 0 7 7 8 7 1 1 3 1 7 8 7 0 2 0\n",
            " 4 1 1 4 8 2 1 8 5 9 7 0 1 4 0 6 5 3 4 7 1 8 0 1 6 4 3 4 7 6 7 8 7 9 9 4 2\n",
            " 7 7 0 9 2 8 4 7 2 0 0 9 9 1 0 5 4 1 5 4 1 9 1 8 6 4 1 6 7 1 6 0 0 1 6 1 1\n",
            " 4 6 4 4 1 1 7 6 4 7 9 1 7 2 0 9 0 9 4 7 4 6 3 7 4 1 4 4 1 0 9 1 0 9 6 8 7\n",
            " 9 9 4 4 7 1 2 0 6 3 2 5 1 2 1 1 0 4 9 4 4 0 7 9 0 4 1 8 4 1 3 1 2 7 4 8 8\n",
            " 1 0 7 7 0 4 7 0 7 8 9 2 9 0 9 1 4 4 9 2 9 2 4 8 4 4 2 4 9 7 4 7 5 4 4 5 4\n",
            " 3 0 2 8 0 0 4 1 2 9 0 1 4 0 4 1 0 8 1 7 4 0 1 6 1 0 4 6 7 8 1 7 9 7 0 7 5\n",
            " 1 9 8 3 1 1 4 8 0 0 4 9 7 7 1 3 2 2 4 4 7 1 2 2 5 4 6 7 6 4 7 7 7 3 3 7 1\n",
            " 7 1 1 7 2 2 3 4 0 3 1 0 1 9 4 9 9 1 7 2 1 1 0 6 4 4 6 1 4 4 4 4 7 1 2 2 5\n",
            " 0 7 9 4 0 9 3 9 1 6 8 0 6 1 9 2 1 6 2 2 6 4 9 5 2 0 4 4 1 2]\n",
            "[2.3020973009466354, 2.301132763925152, 2.29675791759154, 2.1737323019188564, 1.9103757437126843, 1.8909053047789421, 1.8725384997831924, 1.8437917580745977, 1.7998864417412987, 1.7153661273852854, 1.5934241246189955, 1.4908926184181173, 1.4311755245889477, 1.3976332543002081, 1.3721931363093915, 1.3467147671212516, 1.31875604681474, 1.2880675458020379, 1.2567886292156205, 1.2289277305716937, 1.2256120709913325, 1.208850049366838, 1.194845441818246, 1.1838679986364087, 1.1700367252824422, 1.161533830660277, 1.1461943908596541, 1.1150794116638363, 1.1024820715704855, 1.071897790924354, 1.04604913309191, 1.022962808144451, 1.0085244816660763, 0.9879241949583194, 1.1423438779757056, 0.9584461490340773, 0.9833628675379462, 0.9399450383650869, 0.958975647706323, 0.9081553054777982]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_acc(true_y,predicted_y):\n",
        "  print(\"true y shape: \" + str(true_y.shape[0]))\n",
        "  print(\"np.sum(predicted_y == true_y): \" + str(np.sum(predicted_y == true_y)))\n",
        "  return (np.sum(predicted_y == true_y)/true_y.shape[0])"
      ],
      "metadata": {
        "id": "EuFgMOSu0i5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_acc_1= evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 2\")\n",
        "print(\"# of units: 64\")\n",
        "print(\"Accuracy: \" + str(mlp_acc_1))\n"
      ],
      "metadata": {
        "id": "z17Ggt4ffwzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e840ee3b-daa5-4875-8ce4-2b2ae47f7fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true y shape: 400\n",
            "np.sum(predicted_y == true_y): 253\n",
            "--------------------------------\n",
            "# of hidden layers: 1\n",
            "# of units: 128\n",
            "Accuracy: 0.6325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Experiments"
      ],
      "metadata": {
        "id": "f32E5Avqu5IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with 8000\n",
        "model = MLP(hidden_units=64, HL=2, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=8000)\n",
        "output, losses_3 = model.fit(x_train_fn, y_train, optimizer)\n",
        "\n",
        "yh= output.predict(x_test_fn)"
      ],
      "metadata": {
        "id": "8-jqq9iKIQkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96b8169-af22-42f5-a1d6-697735436c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100==> Loss = 2.302169032410297\n",
            "Epoch 200==> Loss = 2.301444603921684\n",
            "Epoch 300==> Loss = 2.2995403510339383\n",
            "Epoch 400==> Loss = 2.2912015344613446\n",
            "Epoch 500==> Loss = 2.139489770208229\n",
            "Epoch 600==> Loss = 1.919475065487825\n",
            "Epoch 700==> Loss = 1.8914866787005722\n",
            "Epoch 800==> Loss = 1.8930506821611384\n",
            "Epoch 900==> Loss = 1.8785875817879578\n",
            "Epoch 1000==> Loss = 1.8545208300405833\n",
            "Epoch 1100==> Loss = 1.8241916922562162\n",
            "Epoch 1200==> Loss = 1.77801948433492\n",
            "Epoch 1300==> Loss = 1.7013605873312665\n",
            "Epoch 1400==> Loss = 1.6042910374438566\n",
            "Epoch 1500==> Loss = 1.5203407222961918\n",
            "Epoch 1600==> Loss = 1.4618189253687444\n",
            "Epoch 1700==> Loss = 1.4185711249359234\n",
            "Epoch 1800==> Loss = 1.3863631094317244\n",
            "Epoch 1900==> Loss = 1.3596389520428767\n",
            "Epoch 2000==> Loss = 1.3371487599816319\n",
            "Epoch 2100==> Loss = 1.3205008243944842\n",
            "Epoch 2200==> Loss = 1.3123652085572943\n",
            "Epoch 2300==> Loss = 1.3052072338912273\n",
            "Epoch 2400==> Loss = 1.2975836209771805\n",
            "Epoch 2500==> Loss = 1.2895834648641533\n",
            "Epoch 2600==> Loss = 1.280986616517558\n",
            "Epoch 2700==> Loss = 1.2715445271243406\n",
            "Epoch 2800==> Loss = 1.2623369522811048\n",
            "Epoch 2900==> Loss = 1.2585295646204888\n",
            "Epoch 3000==> Loss = 1.2528676398817022\n",
            "Epoch 3100==> Loss = 1.2448019019393268\n",
            "Epoch 3200==> Loss = 1.2358489492921014\n",
            "Epoch 3300==> Loss = 1.223905585326614\n",
            "Epoch 3400==> Loss = 1.2094597530671896\n",
            "Epoch 3500==> Loss = 1.1952090917991713\n",
            "Epoch 3600==> Loss = 1.1794297599925512\n",
            "Epoch 3700==> Loss = 1.1705954462214083\n",
            "Epoch 3800==> Loss = 1.1608267713947058\n",
            "Epoch 3900==> Loss = 1.144344451901427\n",
            "Epoch 4000==> Loss = 1.1395142697490197\n",
            "Epoch 4100==> Loss = 1.1399281311605316\n",
            "Epoch 4200==> Loss = 1.1117686831998477\n",
            "Epoch 4300==> Loss = 1.089920410612669\n",
            "Epoch 4400==> Loss = 1.080216697867944\n",
            "Epoch 4500==> Loss = 1.060319548670149\n",
            "Epoch 4600==> Loss = 1.0578616545552721\n",
            "Epoch 4700==> Loss = 1.0147270636376926\n",
            "Epoch 4800==> Loss = 1.0246676710478295\n",
            "Epoch 4900==> Loss = 1.0130408980526573\n",
            "Epoch 5000==> Loss = 1.0161979806404158\n",
            "Epoch 5100==> Loss = 0.9723911168410343\n",
            "Epoch 5200==> Loss = 0.9542376345354783\n",
            "Epoch 5300==> Loss = 0.9754864894234997\n",
            "Epoch 5400==> Loss = 0.9378716294986116\n",
            "Epoch 5500==> Loss = 0.9502430730984774\n",
            "Epoch 5600==> Loss = 0.9462452202507121\n",
            "Epoch 5700==> Loss = 0.9393562779528836\n",
            "Epoch 5800==> Loss = 0.9149864175460691\n",
            "Epoch 5900==> Loss = 0.9121718600966212\n",
            "Epoch 6000==> Loss = 0.9061809076870205\n",
            "Epoch 6100==> Loss = 0.8951057504054587\n",
            "Epoch 6200==> Loss = 0.910786172941352\n",
            "Epoch 6300==> Loss = 0.8894461897555314\n",
            "Epoch 6400==> Loss = 0.878614268248758\n",
            "Epoch 6500==> Loss = 0.8650076419568207\n",
            "Epoch 6600==> Loss = 0.8665275626486592\n",
            "Epoch 6700==> Loss = 0.8532751412344467\n",
            "Epoch 6800==> Loss = 0.8603642315567248\n",
            "Epoch 6900==> Loss = 0.8563146112971304\n",
            "Epoch 7000==> Loss = 0.8648688794753613\n",
            "Epoch 7100==> Loss = 0.853807760284571\n",
            "Epoch 7200==> Loss = 0.8571003041829117\n",
            "Epoch 7300==> Loss = 0.8805046213993739\n",
            "Epoch 7400==> Loss = 0.8695249739785832\n",
            "Epoch 7500==> Loss = 0.8806701908394352\n",
            "Epoch 7600==> Loss = 0.8990164665767284\n",
            "Epoch 7700==> Loss = 0.8568230314965679\n",
            "Epoch 7800==> Loss = 0.9323131938247787\n",
            "Epoch 7900==> Loss = 0.8730123124278234\n",
            "Epoch 8000==> Loss = 0.8868836054250605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_acc(true_y,predicted_y):\n",
        "  print(\"true y shape: \" + str(true_y.shape[0]))\n",
        "  print(\"np.sum(predicted_y == true_y): \" + str(np.sum(predicted_y == true_y)))\n",
        "  return (np.sum(predicted_y == true_y)/true_y.shape[0])\n",
        "\n",
        "mlp_acc_1= evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 2\")\n",
        "print(\"# of units: 64\")\n",
        "print(\"Accuracy: \" + str(mlp_acc_1))"
      ],
      "metadata": {
        "id": "KCwhN_N7piGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32a0a23-13ba-469f-da66-d6bd8b8b1faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true y shape: 400\n",
            "np.sum(predicted_y == true_y): 251\n",
            "--------------------------------\n",
            "# of hidden layers: 2\n",
            "# of units: 64\n",
            "Accuracy: 0.6275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with 8000\n",
        "model = MLP(hidden_units=64, HL=1, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=4000)\n",
        "output, losses_4 = model.fit(x_train_fn, y_train, optimizer)\n",
        "\n",
        "yh_2= output.predict(x_test_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRaSrqTQtL7t",
        "outputId": "f68a4889-ca2b-4c12-c4f9-cda5c916f7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100==> Loss = 0.9352407673058577\n",
            "Epoch 200==> Loss = 0.6923510009046661\n",
            "Epoch 300==> Loss = 0.5367905304631673\n",
            "Epoch 400==> Loss = 0.4408959096271017\n",
            "Epoch 500==> Loss = 0.3684593991823587\n",
            "Epoch 600==> Loss = 0.3182868244074773\n",
            "Epoch 700==> Loss = 0.3115668185914102\n",
            "Epoch 800==> Loss = 0.2430013002558912\n",
            "Epoch 900==> Loss = 0.21357973095702756\n",
            "Epoch 1000==> Loss = 0.18091889992765015\n",
            "Epoch 1100==> Loss = 0.16784292437993298\n",
            "Epoch 1200==> Loss = 0.1573885861744172\n",
            "Epoch 1300==> Loss = 0.1615248510583485\n",
            "Epoch 1400==> Loss = 0.1235048917748485\n",
            "Epoch 1500==> Loss = 0.0932180732849198\n",
            "Epoch 1600==> Loss = 0.07959145556891103\n",
            "Epoch 1700==> Loss = 0.07562179914144367\n",
            "Epoch 1800==> Loss = 0.061751336887682175\n",
            "Epoch 1900==> Loss = 0.054884688138625536\n",
            "Epoch 2000==> Loss = 0.04890900610290354\n",
            "Epoch 2100==> Loss = 0.04353724449600618\n",
            "Epoch 2200==> Loss = 0.03923849771083114\n",
            "Epoch 2300==> Loss = 0.035269843224788704\n",
            "Epoch 2400==> Loss = 0.03206275021283487\n",
            "Epoch 2500==> Loss = 0.029223427579158984\n",
            "Epoch 2600==> Loss = 0.026745416131442823\n",
            "Epoch 2700==> Loss = 0.024609626203302214\n",
            "Epoch 2800==> Loss = 0.02271560381006853\n",
            "Epoch 2900==> Loss = 0.021062928733263\n",
            "Epoch 3000==> Loss = 0.01958988932713212\n",
            "Epoch 3100==> Loss = 0.01826550956382888\n",
            "Epoch 3200==> Loss = 0.017101341481329493\n",
            "Epoch 3300==> Loss = 0.016044177722880755\n",
            "Epoch 3400==> Loss = 0.015098955116008719\n",
            "Epoch 3500==> Loss = 0.014243647168770026\n",
            "Epoch 3600==> Loss = 0.013470544686749377\n",
            "Epoch 3700==> Loss = 0.012759867022571059\n",
            "Epoch 3800==> Loss = 0.012117018255123796\n",
            "Epoch 3900==> Loss = 0.01152182752387133\n",
            "Epoch 4000==> Loss = 0.010979233075134076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_acc(true_y,predicted_y):\n",
        "  print(\"true y shape: \" + str(true_y.shape[0]))\n",
        "  print(\"np.sum(predicted_y == true_y): \" + str(np.sum(predicted_y == true_y)))\n",
        "  return (np.sum(predicted_y == true_y)/true_y.shape[0])\n",
        "\n",
        "mlp_acc_1= evaluate_acc(y_test,yh_2)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 1\")\n",
        "print(\"# of units: 64\")\n",
        "print(\"Accuracy: \" + str(mlp_acc_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JE83tP9uvHk",
        "outputId": "909750ab-a41d-4ede-e049-300fbbe2e6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true y shape: 400\n",
            "np.sum(predicted_y == true_y): 324\n",
            "--------------------------------\n",
            "# of hidden layers: 1\n",
            "# of units: 64\n",
            "Accuracy: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with 8000\n",
        "model = MLP(hidden_units=64, HL=0, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=4000)\n",
        "output, losses_5 = model.fit(x_train_fn, y_train, optimizer)\n",
        "\n",
        "yh_2= output.predict(x_test_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "C8am0zsNoOWC",
        "outputId": "e762bd8c-e4d8-4d6c-d43f-c283a525f451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9a54f0ce94a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#with 8000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(losses_2 , \"\\n\", losses_3, \"\\n\", losses_4 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "L62q0dcR4WyM",
        "outputId": "5be2b581-1860-4d6c-bcda-ce55a421fcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5e21993f05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_4\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'losses_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 3 Different Models\n"
      ],
      "metadata": {
        "id": "Bdb2ybBxu-ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP with no hidden layers, i.e., it directly maps the inputs to outputs\n",
        "model = MLP(M=128, HL=0, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=20000)\n",
        "yh = model.fit(x_train_f, y_train, optimizer).predict(x_test_f)\n",
        "mlp_acc = evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: none\")\n",
        "print(\"# of units: 128\")\n",
        "print(\"Activation function: ReLu\")\n",
        "print(\"Accuracy: \" + str(mlp_acc))\n",
        "\n",
        "#MLP with a single hidden layer having 128 units and ReLU activations\n",
        "model = MLP(M=128, HL=1, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=20000)\n",
        "yh = model.fit(x_train_f, y_train, optimizer).predict(x_test_f)\n",
        "mlp_acc = evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 1\")\n",
        "print(\"# of units: 128\")\n",
        "print(\"Activation function: ReLu\")\n",
        "print(\"Accuracy: \" + str(mlp_acc))\n",
        "\n",
        "#MLP with 2 hidden layers each having 128 units with ReLU activations\n",
        "model = MLP(M=128, HL=2, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=20000)\n",
        "yh = model.fit(x_train_f, y_train, optimizer).predict(x_test_f)\n",
        "mlp_acc = evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 2\")\n",
        "print(\"# of units: 128\")\n",
        "print(\"Activation function: ReLu\")\n",
        "print(\"Accuracy: \" + str(mlp_acc))"
      ],
      "metadata": {
        "id": "7aS-6eTIwULH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Activation Functions"
      ],
      "metadata": {
        "id": "Jssb78nvvP6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function: tanh\n",
        "model = MLP(M=128, HL=2, activation=tanh_act)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=20000)\n",
        "yh = model.fit(x_train_f, y_train, optimizer).predict(x_test_f)\n",
        "mlp_acc = evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 2\")\n",
        "print(\"# of units: 128\")\n",
        "print(\"Activation function: tanh\")\n",
        "print(\"Accuracy: \" + str(mlp_acc))\n",
        "\n",
        "# Activation function: Leaky-ReLu\n",
        "# we probably need training for the parameters of leaky ReLu !!\n",
        "model = MLP(M=128, HL=2, activation=leaky_relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=20000)\n",
        "yh = model.fit(x_train_f, y_train, optimizer).predict(x_test_f)\n",
        "mlp_acc = evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 2\")\n",
        "print(\"# of units: 128\")\n",
        "print(\"Activation function: Leaky ReLu\")\n",
        "print(\"Accuracy: \" + str(mlp_acc))"
      ],
      "metadata": {
        "id": "b9XCqg-6Yn1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dropout Regularization"
      ],
      "metadata": {
        "id": "RTkqeCuiwbVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout during training"
      ],
      "metadata": {
        "id": "bE03x6AIkvwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout during testing"
      ],
      "metadata": {
        "id": "yM8JF5Rhk3J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training with Unnormalized Images"
      ],
      "metadata": {
        "id": "SofMoDjewnzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(M=128, HL=2, activation=relu)\n",
        "optimizer = GradientDescent(learning_rate=.1, max_iters=20000)\n",
        "yh = model.fit(x_train_f, y_train, optimizer).predict(x_test_f)\n",
        "mlp_acc = evaluate_acc(y_test,yh)\n",
        "print(\"--------------------------------\")\n",
        "print(\"# of hidden layers: 2\")\n",
        "print(\"# of units: 128\")\n",
        "print(\"Activation function: ReLu\")\n",
        "print(\"Data: unnormalized\")\n",
        "print(\"Accuracy: \" + str(mlp_acc))"
      ],
      "metadata": {
        "id": "eBBjtZRXmmJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Convolutional Neural Network"
      ],
      "metadata": {
        "id": "-9TCKpgLwtpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "5z3_sgg80sbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b71e5e-3e0f-4905-f11e-fdc7880b056c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 18913215.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 317909.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5562990.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 15184442.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "662pehUs37KA",
        "outputId": "969bce57-9c75-4047-94c3-ffaabf75010f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GcDuc1E50zCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=100, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "ZwnYYiIk06I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataloader)"
      ],
      "metadata": {
        "id": "QaD9nR6n4HVx",
        "outputId": "5da1ab07-0782-4fa9-dc79-2437c9e6a18b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7dded9affcd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_net = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=1, out_channels=3, kernel_size=4, stride=1, padding=2), #output 28x28\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2), #output 14x14\n",
        "          nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, stride=1, padding=1),  #output 14x14\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2), #output 7x7\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(49, 128),\n",
        "          nn.Linear(128, 128),\n",
        "          nn.Linear(128, 10) #output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_net(x)\n",
        "        return out\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "# If GPU is available, move the model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Network Architecture: \\n {model}\")\n",
        "print(f\"The model is at : {device}\")"
      ],
      "metadata": {
        "id": "6g1GabQgXPAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "metadata": {
        "id": "R9KRYfpinhJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training took about 11min\n",
        "losses = []\n",
        "accuracies = []\n",
        "for epoch in range(50):\n",
        "    epoch_loss = []\n",
        "    train_acc = []\n",
        "\n",
        "    # Iterate over data\n",
        "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
        "      #move tensor to the same device (CPU/GPU) as the model\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      # Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + loss calc + backward + step\n",
        "      outputs = model(images)\n",
        "      loss = loss_func(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch_idx % 500 == 0:\n",
        "        print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "      epoch_loss.append(loss.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for images, labels in test_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        #images = images.view(-1, 28*28)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    losses.append(np.mean(epoch_loss))\n",
        "    accuracies.append(100 * correct / total)"
      ],
      "metadata": {
        "id": "l0Vr65e4njW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "plt.title('CNN Loss and Accuracy During Training')\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss', color=color)\n",
        "ax1.plot(np.arange(len(losses)), losses, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
        "ax2.plot(np.arange(len(accuracies)), accuracies, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvBpG7IYwYlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        #images = images.view(-1, 28*28)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {(100 * correct / total)} %\")"
      ],
      "metadata": {
        "id": "NwOU_-GhkMiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. MLP Architecture"
      ],
      "metadata": {
        "id": "cI6oiSQUw3Wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Plots of Results"
      ],
      "metadata": {
        "id": "VLpjEkFPw9bI"
      }
    }
  ]
}